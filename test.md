# 3. 方法

## 3.1 公式和架构概述

我们将条件对齐问题公式化如下：给定一组视觉条件 $\mathcal{C} = \{c_1, c_2, ..., c_N\}$，其中每个 $c_i \in \mathbb{R}^{H \times W \times C_i}$ 代表不同的条件类型（例如，深度图、草图、边缘图）。我们的目标是学习一个统一的映射函数 $\mathcal{F}: c_i \rightarrow c_t$，将任意条件 $c_i$ 转换为目标条件 $c_t \in \mathbb{R}^{H \times W \times C_t}$。这使得下游生成模型能够专门针对 $c_t$ 进行操作，从而无需针对特定条件进行调整。

与以往学习直接确定性映射的方法不同，我们提出通过一个基于分布的框架来建模这种转换，该框架能够捕捉条件空间之间固有的不确定性和多对一关系。我们的条件对齐模块 (CAM) 由四个关键组件组成（图 1）：

1. **条件特定编码器** $E_i:\mathcal{C}_i\rightarrow\mathcal{F}_i$，用于提取针对每种条件类型的特征；
2. **共享编码器** $E_s:\mathcal{F}_i\rightarrow\mathcal{H}$，用于将条件特定特征映射到通用表示空间；
3. **带正则化流的 VAE 瓶颈**，用于将确定性表示转换为结构化的概率潜在空间 $\mathcal{Z}$；
4. **通用解码器** $D:\mathcal{Z}\rightarrow\mathcal{C}_t$，用于从潜在表征重构目标条件。

完整的流程可以表示为：
$$c_t = D(z),\quad\text{其中}\quad z\sim q_\phi(z|c_i)\quad\text{且}\quad q_\phi(z|c_i) = f_\theta(q_0(z_0|E_s(E_i(c_i))))$$

其中，$q_0(z_0|E_s(E_i(c_i)))$表示初始后验分布，$f_\theta$表示正则化流变换。该公式使我们能够通过一个原则性的概率框架来捕捉不同条件类型之间复杂且通常模糊的关系。

## 3.2 条件特定编码器和共享编码器

对于每种条件类型 $c_i$，我们设计一个专门的编码器 $E_i$，用于提取与该条件唯一相关的特征。每个条件特定编码器都遵循类似 U-Net 的下采样结构，并增强了 CBAM 注意力机制 [8]：

$$f_i^l = \text{DownBlock}^l(f_i^{l-1}), \quad l \in \{1,...,L_1\}$$

其中 $f_i^l$ 表示第 $l$ 层的特征图，$f_i^0 = c_i$ 表示输入条件，$L_1$ 表示条件特定编码器中下采样层的数量。每个 $\text{DownBlock}$ 由两个卷积层和一个 CBAM 注意力模块组成：

$$\text{DownBlock}^l(f) = \text{CBAM}(\text{Conv}^l_2(\text{Conv}^l_1(f)))$$

共享编码器 $E_s$ 通过一条通用路径处理来自任何特定条件编码器的特征：

$$h^l = \text{DownBlock}^{L_1+l}(h^{l-1}), \quad l \in \{1,...,L_2\}$$

其中 $h^0 = f_i^{L_1}$ 是特定条件编码器的输出，$L_2$ 是共享编码器中附加下采样层的数量。

为了增强多尺度特征集成，我们结合了特征金字塔网络 (FPN) [9] 和跨尺度特征融合：

$$\{p^l\}_{l=0}^{L_1+L_2} = \text{FPN}(\{f_i^l\}_{l=0}^{L_1} \cup \{h^l\}_{l=0}^{L_2})$$
$$h_{\text{enhanced}} = \text{CrossFusion}(\{p^l\}_{l=0}^{L_1+L_2})$$

最终表示 $h = h^{L_2} + h_{\text{enhanced}}$ 通过残差连接将最深层特征与增强的多尺度特征相结合。

## 3.3 基于流的变分自编码器 (VAE) 中的信息瓶颈

### 3.3.1 信息瓶颈理论

我们方法的核心理论基础是信息瓶颈 (IB) 原理 [10]，它提供了一个用于提取完成任务所需的最小充分统计量的正式框架。在我们的上下文中，我们寻求一种潜在表示 $z$ 满足以下条件：

1. 保留与重建目标条件相关的信息：$I(z; c_t)$
2. 丢弃与源条件无关的细节：$I(z; c_i) - I(z; c_t)$

IB 目标函数可以表示为：

$$\min_{p(z|c_i)} \; I(z; c_i) - \beta I(z; c_t)$$

其中 $\beta$ 控制压缩和保留之间的权衡。该目标函数很难直接优化，但变分方法提供了一个易于处理的近似值 [11]。

### 3.3.2 光流增强变分后验

标准 VAE 将后验分布 $q_\phi(z|x)$ 建模为简单的高斯分布，这限制了它们的表达能力。为了捕捉条件空间之间的复杂关系，我们通过正则化流 [12] 增强了模型，从而实现了更灵活的后验分布。

我们的 VAE 瓶颈首先将共享编码器输出 $h$ 映射到初始高斯后验分布的参数：

$$\mu = f_\mu(h), \quad \log\sigma = f_\sigma(h)$$
$$q_0(z_0|c_i) = \mathcal{N}(z_0; \mu, \sigma^2)$$

其中 $f_\mu$ 和 $f_\sigma$ 是由卷积层和 ELU 激活函数实现的神经网络。

然后，我们通过一系列可逆的正则化流对初始分布进行变换：

$$z_k = f_k(z_{k-1}, h), \quad k \in \{1,...,K\}$$

每个流步骤 $f_k$ 都包含一个 ActNorm 层 [13]，后接一个仿射耦合层 [14]，后者为了提高效率进行了修改：

$$z_{k,1}, z_{k,2} = \text{split}(z_{k-1})$$
$$\log s, t = \text{NN}_k(z_{k,1}, h)$$
$$z_{k,2} = z_{k-2} \odot \exp(\log s) + t$$
$$z_k = \text{concat}(z_{k,1}, z_{k,2})$$

$K$ 之后的变换密度流步骤如下：

$$\log q_K(z_K|c_i) = \log q_0(z_0|c_i) - \sum_{k=1}^{K} \log\left|\det\frac{\partial f_k}{\partial z_{k-1}}\right|$$

这种基于流的方法与标准 VAE 相比具有以下几个关键优势：

1. **表达能力**：流可以将简单分布转换为复杂的多模态分布，从而捕捉不同条件类型之间的复杂关系 [15]。

2. **可处理性**：与其他灵活的后验概率不同，正则化流保持了易于处理的密度评估和采样。

3. **一致性**：流的可逆性质保留了潜在空间的拓扑结构，从而确保了有意义的插值 [16]。

得到的潜在变量 $z = z_K$ 作为解码器的输入，解码器重建目标条件。

### 3.3.3 通用解码器

解码器 $D$ 通过一系列带有跳跃连接的上采样块将潜在表示 $z$ 映射回目标条件空间：

$$g^L = z$$
$$g^{l-1} = \text{UpBlock}^l(g^l, s^{l-1}), \quad l \in \{L, L-1, ..., 1\}$$

其中 $s^{l-1}$ 表示来自编码器的跳跃连接，$g^0 = \hat{c}_t$ 表示重建的目标条件。

## 3.4 训练目标

我们的训练目标将重建保真度与促进结构化潜在空间的正则化项相结合：

$$\mathcal{L} = \mathcal{L}_{\text{recon}} + \lambda_{\text{KL}} \mathcal{L}_{\text{KL}} + \lambda_{z} \mathcal{L}_{z}$$

### 3.4.1 重建损失

我们使用预测目标条件与真实目标条件之间的 L1 距离：

$$\mathcal{L}_{\text{recon}} = \|D(z) - c_t\|_1$$

与 L2 损失相比，L1 损失能够实现更清晰的重建，这对于在边缘和深度不连续等视觉条件下保留结构细节尤为重要。

### 3.4.2 基于流的 KL 散度

为了正则化潜在空间，我们利用流变换后的后验概率和标准高斯先验概率之间的 KL 散度：

$$\mathcal{L}_{\text{KL}} = D_{\text{KL}}(q_K(z|c_i) \| p(z))$$

这可以高效地计算如下：

$$\mathcal{L}_{\text{KL}} = D_{\text{KL}}(q_0(z_0|c_i) \| p(z_0)) - \mathbb{E}_{q_0(z_0|c_i)}\left[\sum_{k=1}^{K} \log\left|\det\frac{\partial f_k}{\partial z_{k-1}}\right|\right]$$

其中第一项是初始高斯后验概率的解析KL函数：

$$D_{\text{KL}}(q_0(z_0|c_i) \| p(z_0)) = \frac{1}{2}\sum_{j}\left(\mu_j^2 + \sigma_j^2 - 1 - \log\sigma_j^2\right)$$

### 3.4.3 直接潜在变量正则化

我们方法的一个关键创新是对潜在变量$z$添加了直接正则化：

$$\mathcal{L}_{z} = \frac{1}{2}\|z\|_2^2$$

这个正则化项直接限制了潜在变量的幅度，使其保持在接近原点的位置。虽然概念简单，但这种直接约束却带来了几个重要的好处：

1. **提升稳定性**：正如 Kumar 等人 [17] 所证明的，直接潜在正则化有助于防止后验崩溃，这是 VAE 中一种常见的故障模式，因为模型会忽略潜在变量。

2. **增强泛化**：通过约束潜在空间，我们提高了容量的利用效率，迫使模型发现更有意义、更可泛化的表示 [18]。

3. **与先验对齐**：该术语明确鼓励转换后的样本与标准样本匹配。d 高斯先验，补充 KL 散度项。

Cemgil 等人 [19] 最近的研究表明，这种对潜在变量的直接正则化可以理解为一种近似推理的形式，能够更好地平衡后验灵活性和计算易处理性之间的权衡。

## 3.5 训练策略

我们通过两个阶段训练 CAM：

1. **基础训练**：首先，我们在一组常见的视觉条件（深度、边缘、法线图等）上训练 CAM，以建立一个能够捕捉跨条件关系的鲁棒潜在空间。

2. **适应**：当需要加入新的条件类型时，我们会冻结共享编码器、VAE 瓶颈和解码器，只训练一个新的特定于条件的编码器。该策略利用在基础训练期间学习到的结构化潜在空间，以最少的数据和训练时间高效地适应新条件。

这种模块化的“热插拔”设计是我们方法的一个关键优势，它能够无缝集成新的条件类型，而不会破坏现有的潜在空间，也无需对下游模型进行重新训练。

总而言之，CAM 提供了一个原则性的框架，通过结构化的概率潜在空间来统一不同的视觉条件。特定条件的编码、共享特征提取、流增强后验建模和直接潜在正则化的结合，实现了稳健的跨条件对齐，并对新的条件类型具有卓越的泛化能力。